# Base model (pulled in step 1)
FROM hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF

# LoRA adapter (ensure this file is in the same directory as this Modelfile,
# or provide the correct relative/absolute path if it's elsewhere)
ADAPTER ./llama-3.2-1b-hermes-fc-adapter-colab-f16.gguf

# Optional: Add parameters, a system message, or a chat template if needed.
# The base model GGUF usually contains a default chat template.
# Example Llama 3 template (if you need to override or specify):
# TEMPLATE """<|begin_of_text|><|start_header_id|>system<|end_header_id|>
# {{ .System }}<|eot_id|><|start_header_id|>user<|end_header_id|>
# {{ .Prompt }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
# {{ .Response }}<|eot_id|>"""
#
# Example parameters:
# PARAMETER temperature 0.7
# PARAMETER top_k 40
# PARAMETER top_p 0.9
#
# Example system message:
# SYSTEM """You are a specialized AI assistant fine-tuned with a Hermes function-calling adapter.""" 